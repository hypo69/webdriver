# Модуль для работы с локальной языковой моделью Llama

## Обзор

Модуль `src.ai.llama.model` предназначен для загрузки и использования локальной языковой модели Llama. Он использует библиотеку `llama_cpp` для взаимодействия с предварительно обученными моделями, хранящимися в формате GGUF. Модуль предоставляет простой способ запуска модели и генерации текста на основе заданного запроса.

## Подробней

Этот модуль позволяет использовать большие языковые модели без необходимости подключения к удаленным серверам, что может быть полезно для задач, требующих конфиденциальности или работы в условиях ограниченного доступа к сети. В текущей реализации модуль загружает модель "Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf" из репозитория "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF" и генерирует текст на основе начальной фразы "Once upon a time,".

## Функции

### `from_pretrained`

```python
llm = Llama.from_pretrained(
	repo_id="lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
	filename="Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
)
```

**Назначение**: Загружает предварительно обученную модель Llama из указанного репозитория.

**Параметры**:
- `repo_id` (str): Идентификатор репозитория, содержащего модель. В данном случае "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF".
- `filename` (str): Имя файла модели в формате GGUF. В данном случае "Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf".

**Как работает функция**:

1.  `Инициализация`: Загружает модель Llama из указанного репозитория и файла, используя библиотеку `llama_cpp`.
2.  `Сохранение модели`: Модель сохраняется в переменной `llm`.

**Примеры**:

```python
llm = Llama.from_pretrained(
	repo_id="lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
	filename="Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
)
```

### `llm`

```python
output = llm(
	"Once upon a time,",
	max_tokens=512,
	echo=True
)
```

**Назначение**: Генерирует текст на основе заданной фразы, используя загруженную модель Llama.

**Параметры**:
- `prompt` (str): Начальная фраза для генерации текста. В данном случае "Once upon a time,".
- `max_tokens` (int): Максимальное количество токенов для генерации. В данном случае 512.
- `echo` (bool): Определяет, нужно ли включать начальную фразу в выходной текст. В данном случае `True`.

**Возвращает**:
- `dict`: Словарь, содержащий сгенерированный текст и метаданные.

**Как работает функция**:

1.  `Передача запроса`: Начальная фраза передается в модель Llama для генерации текста.
2.  `Генерация текста`: Модель генерирует текст на основе начальной фразы, ограничивая количество токенов до 512.
3.  `Формирование результата`: Результат сохраняется в переменной `output`.

**Примеры**:

```python
output = llm(
	"Once upon a time,",
	max_tokens=512,
	echo=True
)
print(output)
```
```
A
↓
Передача запроса модели Llama
↓
Генерация текста
↓
Формирование результата
↓
B
```

```python
output = llm(
	"The capital of Russia is",
	max_tokens=256,
	echo=False
)
print(output)