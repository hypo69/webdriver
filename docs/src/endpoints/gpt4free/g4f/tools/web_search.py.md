# Модуль для веб-поиска

## Обзор

Модуль `web_search.py` предназначен для выполнения веб-поиска и извлечения информации из результатов поиска. Он использует библиотеку `duckduckgo_search` для выполнения поисковых запросов и `BeautifulSoup4` для извлечения текста из веб-страниц. Модуль предоставляет функциональность для очистки и форматирования извлеченного текста, а также кэширование результатов поиска для повышения производительности.

## Подробнее

Этот модуль является частью проекта `hypotez` и используется для предоставления возможности поиска в интернете и обработки результатов поиска. Он может быть использован для получения информации для ответа на вопросы пользователя или для выполнения других задач, требующих доступа к информации в интернете.

Модуль выполняет следующие основные функции:

1.  **Выполнение поиска**: Использует библиотеку `duckduckgo_search` для выполнения поисковых запросов.
2.  **Извлечение текста**: Извлекает текст из веб-страниц, используя библиотеку `BeautifulSoup4`.
3.  **Очистка и форматирование текста**: Очищает и форматирует извлеченный текст для улучшения читаемости и релевантности.
4.  **Кэширование результатов**: Кэширует результаты поиска для повышения производительности и снижения нагрузки на сеть.

## Классы

### `SearchResults`

**Описание**: Класс `SearchResults` представляет собой контейнер для результатов поиска. Он содержит список объектов `SearchResultEntry` и количество использованных слов.

**Принцип работы**:
Класс `SearchResults` инициализируется списком результатов поиска и количеством использованных слов. Он предоставляет методы для итерации по результатам, получения источников и преобразования объекта в словарь.

**Аттрибуты**:

*   `results` (list): Список объектов `SearchResultEntry`, представляющих результаты поиска.
*   `used_words` (int): Количество использованных слов в результатах поиска.

**Методы**:

*   `__init__(self, results: list, used_words: int)`: Инициализирует объект `SearchResults`.
*   `from_dict(cls, data: dict)`: Создает объект `SearchResults` из словаря.
*   `__iter__(self)`: Возвращает итератор по результатам поиска.
*   `__str__(self)`: Возвращает строковое представление результатов поиска.
*   `__len__(self) -> int`: Возвращает количество результатов поиска.
*   `get_sources(self) -> Sources`: Возвращает объект `Sources`, содержащий URL-адреса и заголовки результатов поиска.
*   `get_dict(self)`: Возвращает словарь, представляющий объект `SearchResults`.

### `SearchResultEntry`

**Описание**: Класс `SearchResultEntry` представляет собой отдельный результат поиска. Он содержит заголовок, URL-адрес, сниппет и текст результата.

**Принцип работы**:
Класс `SearchResultEntry` инициализируется заголовком, URL-адресом, сниппетом и текстом результата поиска. Он предоставляет метод для установки текста результата.

**Аттрибуты**:

*   `title` (str): Заголовок результата поиска.
*   `url` (str): URL-адрес результата поиска.
*   `snippet` (str): Сниппет результата поиска.
*   `text` (str): Полный текст результата поиска (может быть `None`).

**Методы**:

*   `__init__(self, title: str, url: str, snippet: str, text: str = None)`: Инициализирует объект `SearchResultEntry`.
*   `set_text(self, text: str)`: Устанавливает текст результата поиска.
*   `get_dict(self)`: Возвращает словарь, представляющий объект `SearchResultEntry`.

## Функции

### `scrape_text`

```python
def scrape_text(html: str, max_words: int = None, add_source=True, count_images: int = 2) -> Iterator[str]:
    """Извлекает текст из HTML-кода веб-страницы.

    Args:
        html (str): HTML-код веб-страницы.
        max_words (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
        add_source (bool, optional): Добавлять ли источник в конце извлеченного текста. По умолчанию `True`.
        count_images (int, optional): Количество изображений для включения в результат. По умолчанию 2.

    Returns:
        Iterator[str]: Итератор по извлеченным текстовым фрагментам.
    """
    ...
```

**Назначение**: Извлекает текст из HTML-кода веб-страницы, удаляя лишние элементы и форматируя результат.

**Параметры**:

*   `html` (str): HTML-код веб-страницы.
*   `max_words` (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
*   `add_source` (bool, optional): Добавлять ли информацию об источнике в конце извлеченного текста. По умолчанию `True`.
*   `count_images` (int, optional): Количество изображений для включения в результат. По умолчанию 2.

**Возвращает**:

*   `Iterator[str]`: Итератор по извлеченным текстовым фрагментам.

**Как работает функция**:

1.  **Преобразование HTML в текст**: Использует `BeautifulSoup` для парсинга HTML и извлечения текста.
2.  **Удаление нежелательных элементов**: Удаляет нежелательные элементы, такие как рекламные блоки и навигационные элементы.
3.  **Извлечение текста из параграфов и заголовков**: Извлекает текст из параграфов, заголовков и других текстовых элементов.
4.  **Ограничение количества слов**: Ограничивает количество извлеченных слов, если указан параметр `max_words`.
5.  **Добавление информации об источнике**: Добавляет информацию об источнике в конце извлеченного текста, если указан параметр `add_source`.
6.  **Извлечение изображений**: Извлекает изображения из HTML-кода и добавляет их в результат.

```
A [Получение HTML]
|
B [Парсинг HTML с BeautifulSoup]
|
C [Удаление нежелательных элементов]
|
D [Извлечение текста из параграфов и заголовков] - E [Извлечение изображений]
|
F [Ограничение количества слов]
|
G [Добавление информации об источнике]
|
H [Вывод итератора по текстовым фрагментам]
```

**Примеры**:

```python
html = "<html><body><h1>Заголовок</h1><p>Текст параграфа.</p></body></html>"
text_iterator = scrape_text(html, max_words=10, add_source=False)
for text in text_iterator:
    print(text)
```

### `fetch_and_scrape`

```python
async def fetch_and_scrape(session: ClientSession, url: str, max_words: int = None, add_source: bool = False) -> str:
    """Асинхронно получает HTML-код веб-страницы и извлекает из него текст.

    Args:
        session (ClientSession): Асинхронная сессия для выполнения HTTP-запросов.
        url (str): URL-адрес веб-страницы.
        max_words (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
        add_source (bool, optional): Добавлять ли информацию об источнике в конце извлеченного текста. По умолчанию `False`.

    Returns:
        str: Извлеченный текст или `None` в случае ошибки.
    """
    ...
```

**Назначение**: Асинхронно загружает HTML-код веб-страницы по заданному URL-адресу и извлекает из него текст, используя функцию `scrape_text`.

**Параметры**:

*   `session` (ClientSession): Асинхронная сессия для выполнения HTTP-запросов.
*   `url` (str): URL-адрес веб-страницы.
*   `max_words` (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
*   `add_source` (bool, optional): Добавлять ли информацию об источнике в конце извлеченного текста. По умолчанию `False`.

**Возвращает**:

*   `str`: Извлеченный текст или `None` в случае ошибки.

**Вызывает исключения**:

*   `ClientError`: Если возникает ошибка при выполнении HTTP-запроса.
*   `asyncio.TimeoutError`: Если превышено время ожидания ответа от сервера.

**Как работает функция**:

1.  **Кэширование**: Проверяет, есть ли кэшированная версия страницы. Если есть - отдает ее
2.  **Выполнение HTTP-запроса**: Выполняет асинхронный HTTP-запрос к заданному URL-адресу.
3.  **Извлечение текста**: Извлекает текст из HTML-кода, используя функцию `scrape_text`.
4.  **Обработка ошибок**: Обрабатывает возможные ошибки, такие как `ClientError` и `asyncio.TimeoutError`.
5.  **Кэширование результата**: Сохраняет результат в кэш

```
A [Проверка наличия кэшированной версии]
|
B [Выполнение HTTP-запроса]
|
C [Извлечение текста с помощью scrape_text]
|
D [Обработка ошибок (ClientError, asyncio.TimeoutError)]
|
E [Кэширование результата]
|
F [Возврат извлеченного текста]
```

**Примеры**:

```python
import asyncio
from aiohttp import ClientSession

async def main():
    async with ClientSession() as session:
        text = await fetch_and_scrape(session, "https://www.example.com", max_words=100)
        if text:
            print(text)

asyncio.run(main())
```

### `search`

```python
async def search(query: str, max_results: int = 5, max_words: int = 2500, backend: str = "auto", add_text: bool = True, timeout: int = 5, region: str = "wt-wt") -> SearchResults:
    """Выполняет поиск в интернете с использованием DuckDuckGo Search.

    Args:
        query (str): Поисковый запрос.
        max_results (int, optional): Максимальное количество результатов для возврата. По умолчанию 5.
        max_words (int, optional): Максимальное количество слов для извлечения из каждой страницы. По умолчанию 2500.
        backend (str, optional): Бэкенд для поиска. По умолчанию "auto".
        add_text (bool, optional): Извлекать ли полный текст из каждой страницы. По умолчанию `True`.
        timeout (int, optional): Время ожидания HTTP-запроса в секундах. По умолчанию 5.
        region (str, optional): Регион для поиска. По умолчанию "wt-wt".

    Returns:
        SearchResults: Объект `SearchResults`, содержащий результаты поиска.
    Raises:
        MissingRequirementsError: Если не установлены необходимые зависимости (`duckduckgo-search` и `beautifulsoup4`).
    """
    ...
```

**Назначение**: Выполняет поиск в интернете, используя DuckDuckGo Search, и возвращает результаты в виде объекта `SearchResults`.

**Параметры**:

*   `query` (str): Поисковый запрос.
*   `max_results` (int, optional): Максимальное количество результатов для возврата. По умолчанию 5.
*   `max_words` (int, optional): Максимальное количество слов для извлечения из каждой страницы. По умолчанию 2500.
*   `backend` (str, optional): Бэкенд для поиска. По умолчанию "auto".
*   `add_text` (bool, optional): Извлекать ли полный текст из каждой страницы. По умолчанию `True`.
*   `timeout` (int, optional): Время ожидания HTTP-запроса в секундах. По умолчанию 5.
*   `region` (str, optional): Регион для поиска. По умолчанию "wt-wt".

**Возвращает**:

*   `SearchResults`: Объект `SearchResults`, содержащий результаты поиска.

**Вызывает исключения**:

*   `MissingRequirementsError`: Если не установлены необходимые зависимости (`duckduckgo-search` и `beautifulsoup4`).

**Как работает функция**:

1.  **Проверка зависимостей**: Проверяет, установлены ли необходимые библиотеки `duckduckgo-search` и `beautifulsoup4`.
2.  **Выполнение поиска**: Выполняет поиск с использованием библиотеки `duckduckgo_search`.
3.  **Извлечение текста**: Извлекает текст из каждой страницы результата поиска, используя функцию `fetch_and_scrape`.
4.  **Форматирование результатов**: Форматирует результаты поиска в виде списка объектов `SearchResultEntry`.
5.  **Ограничение количества слов**: Ограничивает количество слов, извлеченных из каждой страницы, в соответствии с параметром `max_words`.
6.  **Создание объекта `SearchResults`**: Создает объект `SearchResults`, содержащий отформатированные результаты поиска.

```
A [Проверка зависимостей (duckduckgo-search, beautifulsoup4)]
|
B [Выполнение поиска с использованием duckduckgo_search]
|
C [Извлечение текста с использованием fetch_and_scrape]
|
D [Форматирование результатов в виде SearchResultEntry]
|
E [Ограничение количества слов]
|
F [Создание объекта SearchResults]
|
G [Возврат объекта SearchResults]
```

**Примеры**:

```python
import asyncio

async def main():
    results = await search("Python programming", max_results=3)
    for result in results:
        print(f"Title: {result.title}")
        print(f"URL: {result.url}")
        print(f"Snippet: {result.snippet}")
        if result.text:
            print(f"Text: {result.text[:100]}...")

asyncio.run(main())
```

### `do_search`

```python
async def do_search(prompt: str, query: str = None, instructions: str = DEFAULT_INSTRUCTIONS, **kwargs) -> tuple[str, Sources]:
    """Выполняет поиск в интернете и формирует промпт для языковой модели.

    Args:
        prompt (str): Исходный промпт пользователя.
        query (str, optional): Поисковый запрос. Если не указан, используется первая строка промпта. По умолчанию `None`.
        instructions (str, optional): Инструкции для языковой модели. По умолчанию `DEFAULT_INSTRUCTIONS`.
        **kwargs: Дополнительные параметры для функции `search`.

    Returns:
        tuple[str, Sources]: Кортеж, содержащий новый промпт и объект `Sources` с источниками результатов поиска.
    """
    ...
```

**Назначение**: Выполняет поиск в интернете на основе заданного промпта и формирует новый промпт, включающий результаты поиска и инструкции для языковой модели.

**Параметры**:

*   `prompt` (str): Исходный промпт пользователя.
*   `query` (str, optional): Поисковый запрос. Если не указан, используется первая строка промпта. По умолчанию `None`.
*   `instructions` (str, optional): Инструкции для языковой модели. По умолчанию `DEFAULT_INSTRUCTIONS`.
*   `**kwargs`: Дополнительные параметры для функции `search`.

**Возвращает**:

*   `tuple[str, Sources]`: Кортеж, содержащий новый промпт и объект `Sources` с источниками результатов поиска.

**Как работает функция**:

1.  **Проверка наличия инструкций**: Проверяет, не были ли результаты поиска уже добавлены в промпт.
2.  **Определение поискового запроса**: Определяет поисковый запрос, используя первую строку промпта, если запрос не был передан явно.
3.  **Кэширование**: Проверяет, есть ли кэшированные результаты поиска.
4.  **Выполнение поиска**: Выполняет поиск с использованием функции `search`, если результаты не были найдены в кэше.
5.  **Формирование нового промпта**: Формирует новый промпт, включающий результаты поиска, инструкции для языковой модели и исходный промпт пользователя.

```
A [Проверка наличия инструкций в промпте]
|
B [Определение поискового запроса]
|
C [Проверка кэшированных результатов]
|
D [Выполнение поиска (если нет в кэше)]
|
E [Формирование нового промпта]
|
F [Возврат нового промпта и источников]
```

**Примеры**:

```python
import asyncio

async def main():
    prompt = "What is the capital of France?"
    new_prompt, sources = await do_search(prompt)
    print(f"New prompt: {new_prompt}")
    print(f"Sources: {sources}")

asyncio.run(main())
```

### `get_search_message`

```python
def get_search_message(prompt: str, raise_search_exceptions=False, **kwargs) -> str:
    """Выполняет поиск и возвращает сформированный промпт.

    Args:
        prompt (str): Исходный промпт пользователя.
        raise_search_exceptions (bool, optional): Поднимать ли исключения, возникающие при поиске. По умолчанию `False`.
        **kwargs: Дополнительные параметры для функции `do_search`.

    Returns:
        str: Сформированный промпт, включающий результаты поиска.
    """
    ...
```

**Назначение**: Выполняет поиск в интернете и возвращает сформированный промпт, обрабатывая возможные исключения.

**Параметры**:

*   `prompt` (str): Исходный промпт пользователя.
*   `raise_search_exceptions` (bool, optional): Поднимать ли исключения, возникающие при поиске. По умолчанию `False`.
*   `**kwargs`: Дополнительные параметры для функции `do_search`.

**Возвращает**:

*   `str`: Сформированный промпт, включающий результаты поиска.

**Как работает функция**:

1.  **Выполнение поиска**: Выполняет поиск с использованием функции `do_search`.
2.  **Обработка исключений**: Обрабатывает возможные исключения, такие как `DuckDuckGoSearchException` и `MissingRequirementsError`.
3.  **Возврат промпта**: Возвращает сформированный промпт, включающий результаты поиска.

```
A [Выполнение поиска с do_search]
|
B [Обработка исключений (DuckDuckGoSearchException, MissingRequirementsError)]
|
C [Возврат сформированного промпта]
```

**Примеры**:

```python
prompt = "What is the capital of Germany?"
search_message = get_search_message(prompt)
print(search_message)
```

### `spacy_get_keywords`

```python
def spacy_get_keywords(text: str):
    """Извлекает ключевые слова из текста с использованием spaCy.

    Args:
        text (str): Текст для извлечения ключевых слов.

    Returns:
        list: Список ключевых слов.
    """
    ...
```

**Назначение**: Извлекает ключевые слова из текста с использованием библиотеки `spaCy`.

**Параметры**:

*   `text` (str): Текст для извлечения ключевых слов.

**Возвращает**:

*   `list`: Список ключевых слов.

**Как работает функция**:

1.  **Проверка зависимостей**: Проверяет, установлена ли библиотека `spaCy`.
2.  **Загрузка языковой модели**: Загружает языковую модель `en_core_web_sm` из библиотеки `spaCy`.
3.  **Обработка текста**: Обрабатывает текст с использованием языковой модели.
4.  **Извлечение ключевых слов**: Извлекает ключевые слова на основе частей речи и именованных сущностей.
5.  **Удаление дубликатов**: Удаляет дубликаты из списка ключевых слов.

```
A [Проверка наличия spaCy]
|
B [Загрузка языковой модели spaCy]
|
C [Обработка текста с помощью spaCy]
|
D [Извлечение ключевых слов (существительные, прилагательные, именованные сущности)]
|
E [Удаление дубликатов]
|
F [Возврат списка ключевых слов]
```

**Примеры**:

```python
text = "The quick brown fox jumps over the lazy dog."
keywords = spacy_get_keywords(text)
print(keywords)