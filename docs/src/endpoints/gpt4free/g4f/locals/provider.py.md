# Модуль для работы с локальными провайдерами моделей

## Обзор

Этот модуль предоставляет класс `LocalProvider` для создания завершений (completions) с использованием локально установленных моделей, управляемых библиотекой `GPT4All`. Он включает в себя функции для поиска моделей, их загрузки и взаимодействия с ними для генерации ответов на основе предоставленных сообщений. Модуль предназначен для работы в рамках проекта `hypotez` и обеспечивает интеграцию с другими компонентами, такими как `gpt4all` и системой логирования.

## Подробней

Модуль `provider.py` предназначен для работы с локальными моделями, используемыми для генерации ответов. Он позволяет находить, загружать и использовать модели GPT4All. Ключевым компонентом является класс `LocalProvider`, который предоставляет статический метод `create_completion` для генерации текста на основе предоставленных сообщений.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """
    Функция поиска директории модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к директории, содержащей файл модели.
    """
```

**Назначение**:
Функция `find_model_dir` предназначена для поиска директории, в которой расположен файл модели GPT4All. Она выполняет поиск в нескольких предопределенных местах, чтобы найти файл модели.

**Параметры**:
- `model_file` (str): Имя файла модели, которую необходимо найти.

**Возвращает**:
- `str`: Путь к директории, содержащей файл модели. Если файл не найден, возвращает путь к новой директории моделей.

**Как работает функция**:

1. **Определение путей**: Определяет пути к возможным расположениям файла модели: локальная директория, директория проекта, новая директория моделей.
2. **Проверка существования файла**: Проверяет наличие файла модели в каждой из этих директорий.
3. **Поиск в рабочей директории**: Если файл не найден в предопределенных местах, выполняет поиск по всем поддиректориям текущей рабочей директории.
4. **Возврат результата**: Возвращает путь к директории, в которой найден файл модели. Если файл не найден нигде, возвращает путь к новой директории моделей.

```
Определение путей → Проверка существования файла в локальной директории → Проверка существования файла в директории проекта → Поиск в рабочей директории → Возврат пути к директории модели
```

**Примеры**:

```python
model_file = "ggml-model.bin"
model_dir = find_model_dir(model_file)
print(model_dir)  # Вывод: /путь/к/директории/с/моделью
```

## Классы

### `LocalProvider`

```python
class LocalProvider:
    """
    Класс для работы с локальными моделями.
    """
```

**Описание**:
Класс `LocalProvider` предоставляет статический метод для создания завершений (completions) с использованием локально установленных моделей GPT4All.

**Методы**:

#### `create_completion`

```python
@staticmethod
def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
    """
    Создает завершение с использованием локальной модели.

    Args:
        model (str): Имя модели.
        messages (Messages): Список сообщений для модели.
        stream (bool): Флаг стриминга.
        **kwargs: Дополнительные аргументы.

    Returns:
        Generator[str, None, None] | str: Генератор токенов или строка с результатом.
    """
```

**Назначение**:
Метод `create_completion` создает завершение на основе локальной модели GPT4All. Он загружает модель, формирует запрос и генерирует ответ.

**Параметры**:
- `model` (str): Имя модели для использования.
- `messages` (Messages): Список сообщений, представляющих контекст для генерации ответа.
- `stream` (bool): Флаг, определяющий, должен ли ответ генерироваться потоково.
- `**kwargs`: Дополнительные аргументы, которые могут быть переданы модели.

**Возвращает**:
- `Generator[str, None, None] | str`: Генератор токенов, если `stream` установлен в `True`, или строка с полным ответом, если `stream` установлен в `False`.

**Как работает функция**:

1. **Инициализация списка моделей**: Если список моделей не инициализирован, он инициализируется с помощью `get_models()`.
2. **Проверка наличия модели**: Проверяет, есть ли запрошенная модель в списке доступных моделей.
3. **Получение информации о модели**: Получает информацию о модели из списка.
4. **Поиск директории модели**: Использует функцию `find_model_dir` для определения местоположения файла модели.
5. **Загрузка модели**: Загружает модель GPT4All с использованием найденного пути. Если модель не найдена, предлагает пользователю загрузить ее.
6. **Формирование системного сообщения**: Формирует системное сообщение из сообщений с ролью "system".
7. **Формирование запроса**: Формирует запрос для модели на основе предоставленных сообщений.
8. **Генерация ответа**: Генерирует ответ с использованием модели GPT4All. Если `stream` установлен в `True`, генерирует ответ потоково.

```
Инициализация списка моделей → Проверка наличия модели → Получение информации о модели → Поиск директории модели → Загрузка модели → Формирование системного сообщения → Формирование запроса → Генерация ответа
```

**Примеры**:

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"}
]
model_name = "ggml-model.bin"
# Предполагается, что модель ggml-model.bin находится в одной из стандартных директорий
response = LocalProvider.create_completion(model_name, messages, stream=False)
print(response)  # Вывод: Paris