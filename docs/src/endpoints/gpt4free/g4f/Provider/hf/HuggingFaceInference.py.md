# Модуль HuggingFaceInference

## Обзор

Модуль `HuggingFaceInference` предоставляет асинхронный генератор для взаимодействия с моделями Hugging Face Inference API. Он поддерживает различные типы моделей, включая текстовые и графические, и обеспечивает потоковую передачу результатов. Этот модуль интегрируется с провайдером Hugging Face и использует его API для выполнения запросов к моделям.

## Подробней

Этот модуль предназначен для использования в проекте `hypotez` для обеспечения доступа к различным моделям Hugging Face. Он позволяет динамически получать список доступных моделей, обрабатывать запросы к моделям, поддерживать потоковую передачу данных и обрабатывать ошибки, связанные с моделями и API.

## Классы

### `HuggingFaceInference`

**Описание**: Класс `HuggingFaceInference` является асинхронным генератором и миксином провайдера моделей, предназначенным для работы с API Hugging Face Inference.

**Наследует**:
- `AsyncGeneratorProvider`: Предоставляет функциональность асинхронного генератора.
- `ProviderModelMixin`: Предоставляет функциональность для работы с моделями провайдера.

**Атрибуты**:
- `url` (str): URL Hugging Face.
- `parent` (str): Название родительского провайдера ("HuggingFace").
- `working` (bool): Флаг, указывающий на работоспособность провайдера (True).
- `default_model` (str): Модель, используемая по умолчанию.
- `default_image_model` (str): Графическая модель, используемая по умолчанию.
- `model_aliases` (dict): Словарь псевдонимов моделей.
- `image_models` (list): Список графических моделей.
- `model_data` (dict[str, dict]): Кэш данных моделей.

**Методы**:
- `get_models()`: Возвращает список доступных моделей.
- `get_model_data(session: StreamSession, model: str) -> str`: Получает данные о модели из API Hugging Face.
- `create_async_generator(...)`: Создает асинхронный генератор для запросов к API Hugging Face.

## Функции

### `get_models`

```python
@classmethod
def get_models() -> list[str]:
    """Возвращает список доступных моделей из Hugging Face Inference API.

    Получает список моделей из API Hugging Face, фильтрует их по популярности и типу,
    и возвращает объединенный список текстовых и графических моделей.

    Returns:
        list[str]: Список доступных моделей.
    """
```

**Назначение**: Получение списка доступных моделей.

**Как работает функция**:

1.  **Проверка кэша моделей**:
    - Если список моделей уже кэширован (`cls.models`), функция сразу возвращает этот список.
2.  **Инициализация списка моделей**:
    - Создается копия списка текстовых моделей (`text_models.copy()`).
3.  **Получение дополнительных текстовых моделей**:
    - Запрашиваются модели с `inference=warm` и `pipeline_tag=text-generation` из API Hugging Face.
    - Фильтруются модели с `trendingScore >= 10`.
    - Дополнительные модели добавляются в основной список, исключая дубликаты.
4.  **Получение дополнительных графических моделей**:
    - Запрашиваются модели с `pipeline_tag=text-to-image` из API Hugging Face.
    - Фильтруются модели с `trendingScore >= 20`.
    - Дополнительные графические модели добавляются в список графических моделей (`cls.image_models`), исключая дубликаты.
5.  **Объединение списков моделей**:
    - Графические модели добавляются в основной список моделей, исключая дубликаты.
6.  **Кэширование и возврат списка**:
    - Полученный список моделей сохраняется в `cls.models` и возвращается.

**ASCII схема работы функции**:

```
A [Проверка cls.models]
|
B [Если cls.models существует] --> Возврат cls.models
|
C [Если cls.models не существует]
|
D [Копирование text_models]
|
E [Получение extra_models (text)]
|
F [Фильтрация trendingScore >= 10]
|
G [Обновление списка моделей (text)]
|
H [Получение extra_models (image)]
|
I [Фильтрация trendingScore >= 20]
|
J [Обновление списка cls.image_models]
|
K [Добавление image_models в models]
|
L [Сохранение в cls.models и возврат]
```

**Примеры**:

```python
# Пример вызова функции
models = HuggingFaceInference.get_models()
print(models)
```

### `get_model_data`

```python
@classmethod
async def get_model_data(cls, session: StreamSession, model: str) -> str:
    """Асинхронно получает данные о модели из API Hugging Face.

    Args:
        session (StreamSession): Асинхронная сессия для выполнения HTTP-запросов.
        model (str): Название модели.

    Returns:
        str: Данные о модели в формате JSON.

    Raises:
        ModelNotSupportedError: Если модель не поддерживается.
    """
```

**Назначение**: Получение данных о конкретной модели из API Hugging Face.

**Параметры**:

- `session` (StreamSession): Асинхронная сессия для выполнения HTTP-запросов.
- `model` (str): Название модели, данные о которой необходимо получить.

**Возвращает**:

- `str`: Данные о модели в формате JSON.

**Вызывает исключения**:

- `ModelNotSupportedError`: Если модель не поддерживается.

**Как работает функция**:

1.  **Проверка кэша**:
    - Функция проверяет, есть ли данные о модели в кэше `cls.model_data`. Если есть, данные возвращаются из кэша.
2.  **Выполнение HTTP-запроса**:
    - Если данные о модели отсутствуют в кэше, выполняется GET-запрос к API Hugging Face для получения данных о модели.
3.  **Обработка ошибок**:
    - Если статус ответа 404, выбрасывается исключение `ModelNotSupportedError`.
    - В случае других ошибок выполняется проверка статуса ответа с помощью `raise_for_status`.
4.  **Кэширование данных**:
    - Полученные данные о модели сохраняются в кэше `cls.model_data`.
5.  **Возврат данных**:
    - Функция возвращает данные о модели.

**ASCII схема работы функции**:

```
A [Проверка cls.model_data[model]]
|
B [Если данные в кэше] --> Возврат данных из кэша
|
C [Если данных нет в кэше]
|
D [Выполнение GET-запроса к API Hugging Face]
|
E [Обработка ошибок (404)]
|
F [Обработка других ошибок (raise_for_status)]
|
G [Кэширование данных в cls.model_data]
|
H [Возврат данных]
```

**Примеры**:

```python
# Пример вызова функции
import asyncio
from aiohttp import ClientSession

async def main():
    async with ClientSession() as session:
        model_data = await HuggingFaceInference.get_model_data(session, "gpt2")
        print(model_data)

asyncio.run(main())
```

### `create_async_generator`

```python
@classmethod
async def create_async_generator(
    cls,
    model: str,
    messages: Messages,
    stream: bool = True,
    proxy: str = None,
    timeout: int = 600,
    api_base: str = "https://api-inference.huggingface.co",
    api_key: str = None,
    max_tokens: int = 1024,
    temperature: float = None,
    prompt: str = None,
    action: str = None,
    extra_data: dict = {},
    seed: int = None,
    aspect_ratio: str = None,
    width: int = None,
    height: int = None,
    **kwargs
) -> AsyncResult:
    """Создает асинхронный генератор для взаимодействия с API Hugging Face.

    Args:
        model (str): Название модели.
        messages (Messages): Список сообщений для передачи в модель.
        stream (bool): Флаг, указывающий на использование потоковой передачи.
        proxy (str): Адрес прокси-сервера.
        timeout (int): Время ожидания запроса.
        api_base (str): Базовый URL API.
        api_key (str): API-ключ.
        max_tokens (int): Максимальное количество токенов в ответе.
        temperature (float): Температура для генерации текста.
        prompt (str): Дополнительный промпт для генерации изображений.
        action (str): Тип действия ("continue" для продолжения).
        extra_data (dict): Дополнительные данные для запроса.
        seed (int): Зерно для случайной генерации.
        aspect_ratio (str): Соотношение сторон изображения.
        width (int): Ширина изображения.
        height (int): Высота изображения.
        **kwargs: Дополнительные параметры.

    Yields:
        AsyncResult: Часть ответа или результат запроса.

    Raises:
        ModelNotSupportedError: Если модель не поддерживается.
        ResponseError: Если произошла ошибка при обработке ответа.
    """
```

**Назначение**: Создание асинхронного генератора для взаимодействия с API Hugging Face.

**Параметры**:

- `model` (str): Название модели, которую необходимо использовать.
- `messages` (Messages): Список сообщений для передачи в модель.
- `stream` (bool): Флаг, указывающий на использование потоковой передачи данных (по умолчанию `True`).
- `proxy` (str): Адрес прокси-сервера (по умолчанию `None`).
- `timeout` (int): Время ожидания запроса в секундах (по умолчанию 600).
- `api_base` (str): Базовый URL API (по умолчанию `"https://api-inference.huggingface.co"`).
- `api_key` (str): API-ключ для доступа к API (по умолчанию `None`).
- `max_tokens` (int): Максимальное количество токенов в ответе (по умолчанию 1024).
- `temperature` (float): Температура для генерации текста (по умолчанию `None`).
- `prompt` (str): Дополнительный промпт для генерации изображений (по умолчанию `None`).
- `action` (str): Тип действия (`"continue"` для продолжения, по умолчанию `None`).
- `extra_data` (dict): Дополнительные данные для запроса (по умолчанию `{}`).
- `seed` (int): Зерно для случайной генерации (по умолчанию `None`).
- `aspect_ratio` (str): Соотношение сторон изображения (по умолчанию `None`).
- `width` (int): Ширина изображения (по умолчанию `None`).
- `height` (int): Высота изображения (по умолчанию `None`).
- `**kwargs`: Дополнительные параметры.

**Возвращает**:

- `AsyncResult`: Асинхронный генератор, выдающий части ответа или результат запроса.

**Вызывает исключения**:

- `ModelNotSupportedError`: Если модель не поддерживается.
- `ResponseError`: Если произошла ошибка при обработке ответа.

**Как работает функция**:

1.  **Инициализация**:
    - Определение модели с помощью `cls.get_model(model)`.
    - Подготовка заголовков запроса, включая `Accept-Encoding` и `Content-Type`.
    - Если предоставлен `api_key`, добавляется заголовок `Authorization`.
    - Подготовка дополнительных данных для запроса изображений с использованием `use_aspect_ratio`.
2.  **Создание асинхронной сессии**:
    - Создается `StreamSession` с заданными заголовками, прокси и таймаутом.
3.  **Обработка запросов к моделям Together AI**:
    - Если модель находится в списке `provider_together_urls`, формируется запрос к API Together AI для генерации изображений.
    - Формируются данные запроса, включающие `prompt`, `model` и дополнительные параметры изображения.
    - Выполняется POST-запрос к API.
    - Обрабатывается ответ, и извлекаются URL изображений.
    - Возвращается объект `ImageResponse` с URL изображений и промптом.
4.  **Обработка основных запросов**:
    - Если модель не поддерживается Together AI или произошла ошибка, выполняется основной процесс.
    - Подготавливаются параметры запроса, такие как `return_full_text`, `max_new_tokens` и `temperature`.
    - Определяется, является ли действие продолжением предыдущего запроса.
5.  **Получение данных о модели**:
    - Получаются данные о модели с помощью `cls.get_model_data(session, model)`.
    - Определяется тип конвейера (`pipeline_tag`) модели.
6.  **Обработка запросов к моделям генерации изображений**:
    - Если `pipeline_tag` равен `"text-to-image"`, формируется запрос для генерации изображений.
    - Данные запроса включают `inputs` (сформированный промпт) и параметры, такие как `seed` и размеры изображения.
7.  **Обработка запросов к текстовым моделям**:
    - Если `pipeline_tag` равен `"text-generation"` или `"image-text-to-text"`, формируется запрос для генерации текста.
    - Определяется тип модели (`model_type`).
    - Формируются входные данные (`inputs`) с помощью функции `get_inputs`.
    - Если длина входных данных превышает 4096 символов, сообщения усекаются.
    - Если модель имеет тип `"gpt2"` и `max_tokens` больше или равно 1024, `max_new_tokens` устанавливается в 512.
    - Данные запроса включают `inputs`, `parameters` и флаг `stream`.
8.  **Выполнение POST-запроса**:
    - Выполняется POST-запрос к API Hugging Face с сформированными данными.
    - Обрабатывается ответ в зависимости от того, используется ли потоковая передача данных.
9.  **Обработка потоковой передачи данных**:
    - Если `stream` равен `True`, ответ обрабатывается построчно.
    - Каждая строка проверяется на наличие данных (`data:`).
    - Извлекаются данные JSON из строки.
    - Если в данных есть ошибка, выбрасывается исключение `ResponseError`.
    - Извлекается текст из данных и возвращается как часть ответа.
    - Определяется, является ли последний токен специальным (`special`).
    - Возвращается `FinishReason("stop" if is_special else "length")`.
10. **Обработка не потоковой передачи данных**:
    - Если `stream` равен `False`, ответ обрабатывается как единый блок.
    - Сохраняются медиафайлы с помощью `save_response_media`.
    - Извлекается сгенерированный текст из ответа и возвращается.

**ASCII схема работы функции**:

```
A [Инициализация]
|
B [Создание StreamSession]
|
C [Проверка model in provider_together_urls]
|
D [Если True: Запрос к Together AI]
|   |
|   E [Формирование данных запроса]
|   |
|   F [POST-запрос к API]
|   |
|   G [Обработка ответа]
|   |
|   H [Возврат ImageResponse]
|
I [Если False: Основной процесс]
|
J [Подготовка параметров запроса]
|
K [Получение данных о модели (cls.get_model_data)]
|
L [Определение pipeline_tag]
|
M [pipeline_tag == "text-to-image"?]
|   |
|   N [Если True: Формирование запроса для генерации изображений]
|   |
|   O [pipeline_tag in ("text-generation", "image-text-to-text")?]
|   |
|   P [Если True: Формирование запроса для генерации текста]
|   |
|   Q [POST-запрос к API Hugging Face]
|   |
|   R [Обработка ответа (stream=True/False)]
|   |
|   S [Возврат результата]
```

**Примеры**:

```python
# Пример вызова функции
import asyncio
from aiohttp import ClientSession

async def main():
    messages = [{"role": "user", "content": "Напиши короткий стих о космосе."}]
    async with ClientSession() as session:
        generator = HuggingFaceInference.create_async_generator(
            model="gpt2",
            messages=messages,
            stream=True
        )
        async for chunk in generator:
            print(chunk, end="")

asyncio.run(main())
```

### `format_prompt_mistral`

```python
def format_prompt_mistral(messages: Messages, do_continue: bool = False) -> str:
    """Форматирует сообщения в промпт для модели Mistral.

    Args:
        messages (Messages): Список сообщений.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированный промпт.
    """
```

**Назначение**: Форматирование сообщений в промпт для модели Mistral.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированный промпт для модели Mistral.

**Как работает функция**:

1.  **Извлечение системных сообщений**:
    - Извлекаются все сообщения с ролью `"system"` и сохраняются в списке `system_messages`.
2.  **Формирование вопроса**:
    - Последнее сообщение пользователя (`messages[-1]["content"]`) объединяется со всеми системными сообщениями.
3.  **Формирование истории**:
    - История формируется путем объединения сообщений пользователя и ассистента в формате `<s>[INST] {user_message} [/INST] {assistant_message}</s>`.
4.  **Продолжение разговора**:
    - Если `do_continue` равен `True`, удаляется последний тег `</s>` из истории.
5.  **Формирование полного промпта**:
    - Если `do_continue` равен `False`, формируется полный промпт, объединяющий историю и вопрос в формате `{history}\n<s>[INST] {question} [/INST]`.
6.  **Возврат промпта**:
    - Возвращается сформированный промпт.

**ASCII схема работы функции**:

```
A [Извлечение системных сообщений]
|
B [Формирование вопроса]
|
C [Формирование истории]
|
D [do_continue == True?]
|
E [Если True: Удаление последнего тега </s>]
|
F [Формирование полного промпта]
|
G [Возврат промпта]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
prompt = format_prompt_mistral(messages)
print(prompt)
```

### `format_prompt_qwen`

```python
def format_prompt_qwen(messages: Messages, do_continue: bool = False) -> str:
    """Форматирует сообщения в промпт для модели Qwen.

    Args:
        messages (Messages): Список сообщений.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированный промпт.
    """
```

**Назначение**: Форматирование сообщений в промпт для модели Qwen.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированный промпт для модели Qwen.

**Как работает функция**:

1.  **Формирование промпта**:
    - Сообщения объединяются в строку, где каждое сообщение оборачивается тегами `<|im_start|>{role}\n{content}\n<|im_end|>`.
2.  **Продолжение разговора**:
    - Если `do_continue` равен `True`, удаляется последний тег `\n<|im_end|>`.
3.  **Добавление тега ассистента**:
    - Если `do_continue` равен `False`, добавляется тег `<|im_start|>assistant\n`.
4.  **Возврат промпта**:
    - Возвращается сформированный промпт.

**ASCII схема работы функции**:

```
A [Формирование промпта]
|
B [do_continue == True?]
|
C [Если True: Удаление последнего тега]
|
D [Добавление тега ассистента]
|
E [Возврат промпта]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
prompt = format_prompt_qwen(messages)
print(prompt)
```

### `format_prompt_qwen2`

```python
def format_prompt_qwen2(messages: Messages, do_continue: bool = False) -> str:
    """Форматирует сообщения в промпт для модели Qwen2.

    Args:
        messages (Messages): Список сообщений.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированный промпт.
    """
```

**Назначение**: Форматирование сообщений в промпт для модели Qwen2.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированный промпт для модели Qwen2.

**Как работает функция**:

1.  **Формирование промпта**:
    - Сообщения объединяются в строку, где каждое сообщение оборачивается тегами `\\u003C｜{role.capitalize()}｜\\u003E{content}\\u003C｜end of sentence｜\\u003E`.
2.  **Продолжение разговора**:
    - Если `do_continue` равен `True`, удаляется последний тег `\\u003C｜Assistant｜\\u003E`.
3.  **Добавление тега ассистента**:
    - Если `do_continue` равен `False`, добавляется тег `\\u003C｜Assistant｜\\u003E`.
4.  **Возврат промпта**:
    - Возвращается сформированный промпт.

**ASCII схема работы функции**:

```
A [Формирование промпта]
|
B [do_continue == True?]
|
C [Если True: Удаление последнего тега]
|
D [Добавление тега ассистента]
|
E [Возврат промпта]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
prompt = format_prompt_qwen2(messages)
print(prompt)
```

### `format_prompt_llama`

```python
def format_prompt_llama(messages: Messages, do_continue: bool = False) -> str:
    """Форматирует сообщения в промпт для модели Llama.

    Args:
        messages (Messages): Список сообщений.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированный промпт.
    """
```

**Назначение**: Форматирование сообщений в промпт для модели Llama.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированный промпт для модели Llama.

**Как работает функция**:

1.  **Формирование промпта**:
    - Промпт начинается с тега `<|begin_of_text|>`.
    - Сообщения объединяются в строку, где каждое сообщение оборачивается тегами `<|start_header_id|>{role}<|end_header_id|>\n\n{content}\n<|eot_id|>\n`.
2.  **Продолжение разговора**:
    - Если `do_continue` равен `True`, удаляется последний тег `\n<|eot_id|>\n`.
3.  **Добавление тега ассистента**:
    - Если `do_continue` равен `False`, добавляется тег `<|start_header_id|>assistant<|end_header_id|>\n\n`.
4.  **Возврат промпта**:
    - Возвращается сформированный промпт.

**ASCII схема работы функции**:

```
A [Формирование промпта]
|
B [do_continue == True?]
|
C [Если True: Удаление последнего тега]
|
D [Добавление тега ассистента]
|
E [Возврат промпта]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
prompt = format_prompt_llama(messages)
print(prompt)
```

### `format_prompt_custom`

```python
def format_prompt_custom(messages: Messages, end_token: str = "</s>", do_continue: bool = False) -> str:
    """Форматирует сообщения в пользовательский промпт.

    Args:
        messages (Messages): Список сообщений.
        end_token (str): Конечный токен.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированный промпт.
    """
```

**Назначение**: Форматирование сообщений в пользовательский промпт.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `end_token` (str): Конечный токен (по умолчанию `"</s>"`).
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированный пользовательский промпт.

**Как работает функция**:

1.  **Формирование промпта**:
    - Сообщения объединяются в строку, где каждое сообщение оборачивается тегами `<|{role}|>\n{content}{end_token}\n`.
2.  **Продолжение разговора**:
    - Если `do_continue` равен `True`, удаляется последний тег `{end_token}\n`.
3.  **Добавление тега ассистента**:
    - Если `do_continue` равен `False`, добавляется тег `<|assistant|>\n`.
4.  **Возврат промпта**:
    - Возвращается сформированный промпт.

**ASCII схема работы функции**:

```
A [Формирование промпта]
|
B [do_continue == True?]
|
C [Если True: Удаление последнего тега]
|
D [Добавление тега ассистента]
|
E [Возврат промпта]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
prompt = format_prompt_custom(messages, end_token="<|file_separator|>")
print(prompt)
```

### `get_inputs`

```python
def get_inputs(messages: Messages, model_data: dict, model_type: str, do_continue: bool = False) -> str:
    """Форматирует входные данные для модели.

    Args:
        messages (Messages): Список сообщений.
        model_data (dict): Данные о модели.
        model_type (str): Тип модели.
        do_continue (bool): Флаг для указания, нужно ли продолжить предыдущий разговор.

    Returns:
        str: Сформированные входные данные.
    """
```

**Назначение**: Форматирование входных данных для модели.

**Параметры**:

- `messages` (Messages): Список сообщений, которые необходимо отформатировать.
- `model_data` (dict): Данные о модели, используемые для определения формата.
- `model_type` (str): Тип модели (например, `"gpt2"`, `"mistral"`).
- `do_continue` (bool): Флаг, указывающий, нужно ли продолжить предыдущий разговор (по умолчанию `False`).

**Возвращает**:

- `str`: Сформированные входные данные для модели.

**Как работает функция**:

1.  **Определение формата входных данных**:
    - В зависимости от `model_type` и данных о модели (`model_data`) выбирается функция для форматирования входных данных.
    - Для моделей типов `"gpt2"`, `"gpt_neo"`, `"gemma"`, `"gemma2"` используется `format_prompt`.
    - Для модели `"mistral"` от `"mistralai"` используется `format_prompt_mistral`.
    - Для моделей с определенным `eos_token` в `tokenizer_config` используется `format_prompt_custom`, `format_prompt_qwen`, `format_prompt_qwen2` или `format_prompt_llama`.
    - Если ни одно из условий не выполнено, используется `format_prompt`.
2.  **Форматирование входных данных**:
    - Выбранная функция форматирования применяется к сообщениям и флагу `do_continue`.
3.  **Возврат входных данных**:
    - Возвращаются сформированные входные данные.

**ASCII схема работы функции**:

```
A [Определение model_type]
|
B [Выбор функции форматирования]
|
C [Форматирование входных данных]
|
D [Возврат входных данных]
```

**Примеры**:

```python
# Пример вызова функции
messages = [
    {"role": "system", "content": "Ты - полезный ассистент."},
    {"role": "user", "content": "Как дела?"},
    {"role": "assistant", "content": "У меня все хорошо, спасибо!"},
    {"role": "user", "content": "Что ты умеешь?"}
]
model_data = {"config": {"model_type": "gpt2"}}
model_type = "gpt2"
inputs = get_inputs(messages, model_data, model_type)
print(inputs)