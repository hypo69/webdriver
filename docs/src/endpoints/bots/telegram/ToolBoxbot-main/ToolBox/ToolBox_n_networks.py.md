# Модуль для работы с нейронными сетями
======================================

Модуль содержит класс :class:`neural_networks`, который предоставляет методы для взаимодействия с различными нейронными сетями, включая FLUX.1-schnell, Mistral Large 2407 и Free GPT-4o mini.

## Обзор

Этот модуль предназначен для упрощения работы с различными API нейронных сетей. Он предоставляет методы для генерации изображений и обработки текстовых запросов. Модуль использует переменные окружения для хранения токенов доступа к API, что позволяет безопасно взаимодействовать с нейронными сетями.

## Подробней

Модуль содержит класс `neural_networks`, который включает в себя методы для взаимодействия с различными API нейронных сетей. Он использует библиотеки `requests`, `json`, `os`, `io` и `PIL` для выполнения HTTP-запросов, обработки JSON-ответов, работы с файловой системой и обработки изображений соответственно.

## Классы

### `neural_networks`

**Описание**: Класс для взаимодействия с различными нейронными сетями.

**Принцип работы**:
Класс `neural_networks` предоставляет методы для отправки запросов к различным API нейронных сетей и обработки полученных ответов. Он использует переменные окружения для хранения токенов доступа к API, что обеспечивает безопасное взаимодействие с нейронными сетями.

**Методы**:

*   `_FLUX_schnell(self, prompt: str, size: list[int, int], seed: int, num_inference_steps: int) -> str|None`
*   `__mistral_large_2407(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]|str`
*   `_free_gpt_4o_mini(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]|str`

## Функции

### `_FLUX_schnell`

```python
def _FLUX_schnell(self, prompt: str, size: list[int, int], seed: int, num_inference_steps: int) -> str|None:
    """
    Отправляет запрос к API FLUX.1-schnell для генерации изображения.

    Args:
        prompt (str): Текстовое описание желаемого изображения.
        size (list[int, int]): Размеры изображения в виде списка [ширина, высота].
        seed (int): Зерно для генерации случайных чисел.
        num_inference_steps (int): Количество шагов для генерации изображения.

    Returns:
        str | None: Объект Image, представляющий сгенерированное изображение, или None в случае ошибки.
    """
```

**Назначение**: Отправляет запрос к API FLUX.1-schnell для генерации изображения на основе заданного текстового описания, размеров, зерна и количества шагов.

**Параметры**:

*   `prompt` (str): Текстовое описание желаемого изображения.
*   `size` (list[int, int]): Размеры изображения в виде списка [ширина, высота].
*   `seed` (int): Зерно для генерации случайных чисел.
*   `num_inference_steps` (int): Количество шагов для генерации изображения.

**Возвращает**:

*   `str | None`: Объект Image, представляющий сгенерированное изображение, или None в случае ошибки.

**Как работает функция**:

1.  Формирует JSON-payload с входными данными, включающими текстовое описание `prompt`, параметры генерации изображения (`guidance_scale`, `num_inference_steps`, `width`, `height`) и зерно `seed`.
2.  Последовательно выполняет HTTP POST-запросы к API "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell", используя разные токены доступа из переменных окружения `HF_TOKEN{i}`, где `i` принимает значения от 1 до 6.
3.  Проверяет статус код ответа. Если статус код равен 200, функция считает запрос успешным и пытается открыть изображение из полученного содержимого ответа.
4.  В случае успешного открытия изображения, функция возвращает объект `Image`. Если в процессе выполнения запроса возникает ошибка или статус код не равен 200, функция возвращает `None`.

**ASCII flowchart**:

```
A: Формирование payload
|
B: Цикл по токенам HF_TOKEN1..HF_TOKEN6
|
C: Отправка POST-запроса к API FLUX.1-schnell
|
D: Проверка status_code == 200?
|   Нет --> B (следующий токен)
Да
|
E: Открытие изображения из response.content
|
F: Возврат объекта Image
```

Где:

*   `A`: Формирование payload с входными данными.
*   `B`: Цикл по токенам `HF_TOKEN1`..`HF_TOKEN6`.
*   `C`: Отправка POST-запроса к API FLUX.1-schnell.
*   `D`: Проверка `status_code == 200`.
*   `E`: Открытие изображения из `response.content`.
*   `F`: Возврат объекта `Image`.

**Примеры**:

```python
# Пример вызова функции _FLUX_schnell
prompt = "A futuristic cityscape"
size = [512, 512]
seed = 42
num_inference_steps = 50
image = neural_networks()._FLUX_schnell(prompt, size, seed, num_inference_steps)
if image:
    image.show()
else:
    print("Failed to generate image")
```

### `__mistral_large_2407`

```python
def __mistral_large_2407(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]|str:
    """
    Отправляет запрос к API Mistral Large 2407 для обработки текстового запроса.

    Args:
        prompt (list[dict[str, str]]): Список словарей с текстовыми запросами.

    Returns:
        tuple[str, int, int] | str: Кортеж, содержащий ответ, количество токенов в запросе и количество токенов в ответе, или строку в случае ошибки.
    """
```

**Назначение**: Отправляет запрос к API Mistral Large 2407 для обработки текстового запроса и возвращает ответ, а также информацию об использовании токенов.

**Параметры**:

*   `prompt` (list[dict[str, str]]): Список словарей с текстовыми запросами.

**Возвращает**:

*   `tuple[str, int, int] | str`: Кортеж, содержащий ответ, количество токенов в запросе и количество токенов в ответе, или строку в случае ошибки.

**Как работает функция**:

1.  Формирует JSON-payload с входными данными, включающими текстовые запросы `prompt`, параметры генерации текста (`temperature`, `top_p`, `max_tokens`) и модель (`pixtral-12b-2409`).
2.  Выполняет HTTP POST-запрос к API "https://api.mistral.ai/v1/chat/completions" с использованием токена доступа из переменной окружения `MISTRAL_TOKEN`.
3.  Преобразует JSON-ответ в словарь.
4.  Извлекает из ответа текст сообщения, количество токенов в запросе и количество токенов в ответе.
5.  Возвращает кортеж, содержащий текст сообщения, количество токенов в запросе и количество токенов в ответе.

**ASCII flowchart**:

```
A: Формирование payload
|
B: Отправка POST-запроса к API Mistral Large 2407
|
C: Преобразование JSON-ответа в словарь
|
D: Извлечение текста сообщения, количества токенов в запросе и количестве токенов в ответе
|
E: Возврат кортежа (текст сообщения, количество токенов в запросе, количество токенов в ответе)
```

Где:

*   `A`: Формирование payload с входными данными.
*   `B`: Отправка POST-запроса к API Mistral Large 2407.
*   `C`: Преобразование JSON-ответа в словарь.
*   `D`: Извлечение текста сообщения, количества токенов в запросе и количества токенов в ответе.
*   `E`: Возврат кортежа (текст сообщения, количество токенов в запросе, количество токенов в ответе).

**Примеры**:

```python
# Пример вызова функции __mistral_large_2407
prompt = [{"role": "user", "content": "What is the capital of France?"}]
response, prompt_tokens, completion_tokens = neural_networks().__mistral_large_2407(prompt)
print(f"Response: {response}")
print(f"Prompt tokens: {prompt_tokens}")
print(f"Completion tokens: {completion_tokens}")
```

### `_free_gpt_4o_mini`

```python
def _free_gpt_4o_mini(self, prompt: list[dict[str, str]]) -> tuple[str, int, int]|str:
    """
    Отправляет запрос к API Free GPT-4o mini для обработки текстового запроса.

    Args:
        prompt (list[dict[str, str]]): Список словарей с текстовыми запросами.

    Returns:
        tuple[str, int, int] | str: Кортеж, содержащий ответ, количество токенов в запросе и количество токенов в ответе, или результат вызова __mistral_large_2407 в случае ошибки.
    """
```

**Назначение**: Отправляет запрос к API Free GPT-4o mini для обработки текстового запроса и возвращает ответ, а также информацию об использовании токенов. В случае ошибки использует API Mistral Large 2407.

**Параметры**:

*   `prompt` (list[dict[str, str]]): Список словарей с текстовыми запросами.

**Возвращает**:

*   `tuple[str, int, int] | str`: Кортеж, содержащий ответ, количество токенов в запросе и количество токенов в ответе, или результат вызова `__mistral_large_2407` в случае ошибки.

**Как работает функция**:

1.  Формирует JSON-payload с входными данными, включающими текстовые запросы `prompt`, параметры генерации текста (`temperature`, `top_p`, `max_tokens`) и модель (`gpt-4o-mini`).
2.  Последовательно выполняет HTTP POST-запросы к API "https://models.inference.ai.azure.com/chat/completions", используя разные токены доступа из переменных окружения `GIT_TOKEN{i}`, где `i` принимает значения от 1 до 6.
3.  Проверяет статус код ответа. Если статус код равен 200, функция считает запрос успешным и преобразует JSON-ответ в словарь.
4.  Извлекает из ответа текст сообщения, количество токенов в запросе и количество токенов в ответе.
5.  Возвращает кортеж, содержащий текст сообщения, количество токенов в запросе и количество токенов в ответе.
6.  Если в процессе выполнения запроса возникает ошибка или статус код не равен 200, функция вызывает метод `__mistral_large_2407` с тем же запросом и возвращает результат его выполнения.

**ASCII flowchart**:

```
A: Формирование payload
|
B: Цикл по токенам GIT_TOKEN1..GIT_TOKEN6
|
C: Отправка POST-запроса к API Free GPT-4o mini
|
D: Проверка status_code == 200?
|   Нет --> B (следующий токен)
Да
|
E: Преобразование JSON-ответа в словарь
|
F: Извлечение текста сообщения, количества токенов в запросе и количестве токенов в ответе
|
G: Возврат кортежа (текст сообщения, количество токенов в запросе, количество токенов в ответе)
|
H: Вызов __mistral_large_2407(prompt) (если status_code != 200)
```

Где:

*   `A`: Формирование payload с входными данными.
*   `B`: Цикл по токенам `GIT_TOKEN1`..`GIT_TOKEN6`.
*   `C`: Отправка POST-запроса к API Free GPT-4o mini.
*   `D`: Проверка `status_code == 200`.
*   `E`: Преобразование JSON-ответа в словарь.
*   `F`: Извлечение текста сообщения, количества токенов в запросе и количестве токенов в ответе.
*   `G`: Возврат кортежа (текст сообщения, количество токенов в запросе, количество токенов в ответе).
*   `H`: Вызов `__mistral_large_2407(prompt)` (если `status_code != 200`).

**Примеры**:

```python
# Пример вызова функции _free_gpt_4o_mini
prompt = [{"role": "user", "content": "Translate 'Hello, world!' to French."}]
response, prompt_tokens, completion_tokens = neural_networks()._free_gpt_4o_mini(prompt)
print(f"Response: {response}")
print(f"Prompt tokens: {prompt_tokens}")
print(f"Completion tokens: {completion_tokens}")